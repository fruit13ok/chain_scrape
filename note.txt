// to begin this project
mkdir chain_scrape
cd chain_scrape
code .
npm init -y
npm install express puppeteer body-parser node-fetch cors --save
npm install nodemon --save-dev

// in package.json
    "start-dev": "nodemon server"

// create basic back end and front end directories and files
mkdir server
touch server/index.js
mkdir public
touch public/app.js public/index.html

// 
test basic GET POST code 

// run the server
npm run start-dev

// Without front end is ok, this is a back end only project that produce JSON result,
// This front end got redirected the path is for testing, everything can be test with Postman.



// changes for deploy to digital ocean
// in app.js
    // let backendRoute = new URL("http://localhost:8000/api");
    let backendRoute = new URL("http://138.68.234.14:8000/api");
// in index.js
    // const browser = await puppeteer.launch();
    const browser = await puppeteer.launch({args: ['--no-sandbox', '--disable-setuid-sandbox']});

// github repo
echo "# chain_scrape" >> README.md
touch .gitignore
// in .gitignore
    # dependencies
    /node_modules
git init
git add .
git commit -m "first commit"
git remote add origin https://github.com/fruit13ok/chain_scrape.git
git push -u origin master

// deploy to digital ocean
login to droplet
ssh root@138.68.234.14
<password>
git clone https://github.com/fruit13ok/chain_scrape.git
cd chain_scrape
npm install
// test run (will end when logout droplet)
node server
go to http://138.68.234.14:8000/
click details, click visit this unsafe site
// end server
control-c
// run in the background as a service (will keep running after logout droplet)
// check pm2, and quit
pm2 monit
control-c
// run server with pm2
pm2 start server/index.js
// logout droplet
exit
go to http://138.68.234.14:8000/
// after change serverside code, either reload or restart server
// have more uptime, at least keep one service running, restart services one by one
pm2 reload server/index.js
// restart all
pm2 restart server/index.js



/////////// 5-9-2020 ////////////

revisit after a while
Problem:
    I found the code no longer working when searching more than one level deep.

Solution:
    Code deprecation, calling async function now require to end with a catch.
    https://thecodebarbarian.com/unhandled-promise-rejections-in-node.js.html
    EX:
    myAsyncFunction().then().catch(() => {});


// implement 3 search scrape, return as array of object with search key and its results

// implement all links scrape, return all links of the page and status code, convert to PDF
    npm install jspdf jspdf-autotable --save
    // also use CDN in html file