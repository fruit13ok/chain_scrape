// to begin this project
mkdir chain_scrape
cd chain_scrape
code .
npm init -y
npm install express puppeteer body-parser node-fetch cors abort-controller --save
npm install nodemon --save-dev

// in package.json
    "start-dev": "nodemon server"

// create basic back end and front end directories and files
mkdir server
touch server/index.js
mkdir public
touch public/app.js public/index.html

// 
test basic GET POST code 

// run the server
npm run start-dev

// Without front end is ok, this is a back end only project that produce JSON result,
// This front end got redirected the path is for testing, everything can be test with Postman.



// changes for deploy to digital ocean
// in app.js
    // let backendRoute = new URL("http://localhost:8000/api");
    let backendRoute = new URL("http://165.232.52.237:8000/api");
// in index.js
    // const browser = await puppeteer.launch();
    const browser = await puppeteer.launch({args: ['--no-sandbox', '--disable-setuid-sandbox']});

// github repo
echo "# chain_scrape" >> README.md
touch .gitignore
// in .gitignore
    # dependencies
    /node_modules
git init
git add .
git commit -m "first commit"
git remote add origin https://github.com/fruit13ok/chain_scrape.git
git push -u origin master

// deploy to digital ocean
login to droplet
ssh root@165.232.52.237
<password>
git clone https://github.com/fruit13ok/chain_scrape.git
cd chain_scrape
npm install
// test run (will end when logout droplet)
node server
go to http://165.232.52.237:8000/
click details, click visit this unsafe site
// end server
control-c
// run in the background as a service (will keep running after logout droplet)
// check pm2, and quit
pm2 monit
control-c
// run server with pm2
pm2 start server/index.js
// logout droplet
exit
go to http://165.232.52.237:8000/
// after change serverside code, either reload or restart server
// have more uptime, at least keep one service running, restart services one by one
pm2 reload server/index.js
// restart all
pm2 restart server/index.js



/////////// 5-9-2020 ////////////

revisit after a while
Problem:
    I found the code no longer working when searching more than one level deep.

Solution:
    Code deprecation, calling async function now require to end with a catch.
    https://thecodebarbarian.com/unhandled-promise-rejections-in-node.js.html
    EX:
    myAsyncFunction().then().catch(() => {});


// implement 3 search scrape, return as array of object with search key and its results

// implement all links scrape, return all links of the page and status code, convert to PDF
    npm install jspdf jspdf-autotable --save
    // also use CDN in html file

// implement JSON to CSV
    // see all_links repo

// refactor JSON to CSV into function ^_^
// now part 1 and part 4 of scrape have downloadable PDF and CSV


// implement backlinks scrape, return all links from google search with "verbatim" option.
// see my "backlinks" repo for more info
// NOTE: some how this scarpe has "recaptcha" issue, 
// To address "recaptcha" issue, I used "user-agents", "proxy server", and "puppeteer filter"
npm install user-agents --save

/////////////////// this version overview ///////////////////
-took care off recaptcha by not using url parameter to search,
-took care off many request cases,
-same search will cause url status codes '408 Request Timeout' on good links,

//////////////////////////////////////////////////////
backlinks, fix missing early result, not wait before loop

// in wordCountObj() insert 2 properties to an array of objects, totalCount, and percentage


//////////////////////////////////////////////////////
updated backlines to match small google codebase update
added timeout error feedback as frontend response
updated app.js frontend old jQuery code


////////////////////////// added new scrape (single page app) ////////////////////////////
see my repo: https://github.com/fruit13ok/gmap_places
added google maps scrape (hard to scrape)
scrape businesses data from google maps search results
google maps is a isngle page app, dynamicaly generate content
need to slow down the scrape process and not block any resouce type
it scrapes about 5 seconds per 1 content result, easily take over 5 minutes

I updated the Google map business scrape. It is working on Digital Ocean. 
changes: now only scrape the first page’s list of results, not continue on to next page. 
It has included the “category”, “mapID”, and “businesshours”. 
I still apply some delay to the scraping


/////////////////////////// fix this next time ///////////////////////////
found that PDF, and CSV might be outdated


////////////////////////////////////////////////////////
on part 1, added frontend only filter, result can be filter by count


//////////////////////////////////////////////////////////////
part 4 add scrape xml sitemap page


//////////////////////////////////////////////////////////////
change part 4, used Puppeteer to scrape number of H1 and H2 and "goto" response "_status" for status code


//////////////////////////////////////////////////////////////
add part 8 wordcount